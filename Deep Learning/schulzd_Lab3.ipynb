{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Basics of Feed-Forward Neural Networks\n",
    "\n",
    "In this lab, we will start to create a feed-forward neural network from scratch.\n",
    "We begin with the very basic computational unit, a perceptron,\n",
    "and then we will add more layers and increase the complexity of our network. Along the way, we will learn how a perceptron works, the benefits of adding more layers, the kind of transformations necessary for learning complex features and relationships from data, and why an object-oriented paradigm is useful for easier management of our neural network framework.\n",
    "\n",
    "We will implement everything from scratch in Python using helpful\n",
    "libraries such as NumPy and PyTorch (without using the autograd feature of PyTorch). The purpose of this lab and the following lab\n",
    "series is to learn how neural networks work starting from the most basic\n",
    "computational units and proceeding to deeper and more networks. This will help us better understand how other popular deep learning frameworks, such as PyTorch, work underneath. You should be able to easily understand and implement everything in this lab. If you are having trouble consult with your instructors as the next lab series will assume a perfect understanding of the basic feed-forward neural network material.\n",
    "\n",
    "The recommended Python version for this implementation is 3.7. Recommended reading: sections 4.1 and 4.2 of the book (https://www.d2l.ai/chapter_multilayer-perceptrons/index.html).\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "A perceptron or artificial neuron is the most basic processing unit of feed-forward neural networks. A perceptron can be modeled as a single-layer neural network with an input vector $\\mathbf{x} \\in \\mathbb{R}^n$, a bias $b$, a vector of trainable weights $\\mathbf{w} \\in \\mathbb{R}^n$, and an output unit $y$. Given the input $\\mathbf{x}$, the output $y$ is computed by an activation function $f(\\cdot)$ as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "y (\\mathbf{x}; \\Theta) = f\\left(\\sum_{i=1}^{n} x_i w_i + b \\right) = f(\\mathbf{w}^\\intercal \\mathbf{x} + b)\\,,\n",
    "\\end{equation}\n",
    "where $\\Theta = \\{\\mathbf{w}, b\\}$ represents the trainable parameter set. \n",
    "\n",
    "The figure below shows a schematic view of a single output perceptron. Each input value $x_i$ is multiplied by a weight factor $w_i$. The weighted sum added to the bias is then passed through an activation function to obtain the output, $y$.\n",
    " \n",
    "The vector $\\mathbf{x}$ represents one sample of our data and each element $x_i$ represents a feature. Thus, $\\mathbf{x}$ is often referred to as a feature vector. These features can represent different measurements depending on the application. For example, if we are trying to predict if a patient is at high risk of cardiac disease then each element of $\\mathbf{x}$ might contain vital signs such as diastolic and systolic blood pressure, heart rate, blood sugar levels, etc. In another application where we are trying to predict if a tissue biopsy is cancerous or not using mid-infrared imaging then each element of $\\mathbf{x}$ can represent the amount of mid-infrared light absorbed at a particular wavelength. The output $y$ in the applications above could contain values of $0$ or $1$, indicating if the patient is at high risk of cardiac disease or if the tissue biopsy is cancerous or not.\n",
    " \n",
    "Now, let us begin implementing our first artificial neuron.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Let's assume that our feature vector contains measurements of body temperature pressure, pulse oximeter reading, and presence of cough or not. Then for a 'healthy' patient our input sample might look like $\\mathbf{x} = \\begin{bmatrix} 98.6 \\\\ 95 \\\\ 0 \\end{bmatrix}$. Let's say that we are trying to 'predict' the probability of a patient being positive with COVID-19 based on the above measurements.\n",
    "\n",
    "Each element of our input vector is associated with a unique weight. Let the vector of weights be $\\mathbf{w} = \\begin{bmatrix} 0.03 \\\\ 0.55 \\\\ 0.88 \\end{bmatrix}$. Each artifical neuron is also associated with a unique bias. Let the bias be $b = 2.9$. Assuming a linear activation function write the code to produce and print the output $y$ given the above input vector $\\mathbf{x}$, weights $\\mathbf{w}$, and the bias $b$ using the above perceptron model. Do not use any NumPy or PyTorch functions. Use a Python variables for each mathematical variable and use Python lists for vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.858, 55.150000000000006, 2.9]\n"
     ]
    }
   ],
   "source": [
    "x = [98.6, 95, 0]\n",
    "w = [0.03, 0.55, 0.88]\n",
    "b = 2.9\n",
    "y = []\n",
    "for i in range(len(x)):\n",
    "    y.append(x[i] * w[i] + b)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> Question 1: How many parameters does our simple model contain? Be specific.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: The model contains 6 parameters. There are 3 neurons, one for each input value, and one weight value and one bias value for each neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue> Question 2: Recall that we were hoping to 'predict' the probability of a patient being positive with COVID-19. Does the output make sense? If not, elaborate on how you could fix it.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: No it does not, since it doesn't actually tell us anything about a probability of a certain outcome. An output layer with one neuron needs to be added to translate the 3 calculated values into a single probability value that ranges from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron with Multiple Outputs\n",
    "\n",
    "The perceptron model above has only one output. However, in most applications, we need multiple outputs. For example, in a classification problem, we would expect the model to output a vector $\\mathbf{y}$, where each $y_i$ represents the probability of a sample belonging to a particular class $i$. The figure below shows a schematic view of a multiple output feed-forward neural network. Each input value $x_i$ is multiplied by a weight factor $W_{ij}$, where $W_{ij}$ denotes a connection weight between the input node $x_i$ and the output node $y_j$. The weighted sum is added to the bias and then passed through an activation function to obtain the output, $y_j$.\n",
    "\n",
    "Given an input $\\mathbf{x} \\in \\mathbb{R}^n$ this can be modeled as:\n",
    "\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f\\left(\\sum_{i=1}^{n} x_i W_{ij} + b_j\\right) = f(\\mathbf{w}_j^\\intercal \\mathbf{x} + b_j)\\,,\n",
    "\\end{equation}\n",
    "where the parameter set here is $\\Theta = \\{ \\mathbf{W} \\in \\mathbb{R}^{n \\times m}, \\mathbf{b} \\in \\mathbb{R}^m \\}$ and $\\mathbf{w}_j$ denotes the $j^{th}$ column of $\\mathbf{W}$. \n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "\n",
    "Let $\\mathbf{x} = \\begin{bmatrix} 98.6 \\\\ 95 \\\\ 0 \\\\ 1 \\end{bmatrix}$. Let the output vector $\\mathbf{y} \\in \\mathbb{R}^3$, i.e. consisting of $3$ outputs. Let the weights associated with each output node $y_i$ be $\\mathbf{w_1} = \\begin{bmatrix} 0.03 \\\\ 0.55 \\\\ 0.88 \\\\0.73 \\end{bmatrix}$, $\\mathbf{w_2} = \\begin{bmatrix} 0.48 \\\\ 0.31 \\\\ 0.28 \\\\ -0.9 \\end{bmatrix}$, $\\mathbf{w_3} = \\begin{bmatrix} 0.77 \\\\ 0.59 \\\\ 0.37 \\\\ 0.44 \\end{bmatrix}$. Let the bias vector be $\\mathbf{b} = \\begin{bmatrix} 2.9 \\\\ 6.1 \\\\ 3.3 \\end{bmatrix}$. Note that a single bias is associated with each output node $y_i$.\n",
    "\n",
    "Given the above inputs write the code to print the output vector $\\mathbf{y}$.  Use a Python variables for each mathematical variable and use Python lists for vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58.108000000000004, 82.87799999999999, 135.272]\n"
     ]
    }
   ],
   "source": [
    "x = [98.6, 95, 0, 1]\n",
    "W1 = [0.03, 0.55, 0.88, 0.73]\n",
    "W2 = [0.48, 0.31, 0.28, -0.9]\n",
    "W3 = [0.77, 0.59, 0.37, 0.44]\n",
    "W = [W1, W2, W3]\n",
    "b = [2.9, 6.1, 3.3]\n",
    "y = []\n",
    "for i in range(len(W)):\n",
    "    Wx = 0\n",
    "    for j in range(len(W)):\n",
    "        Wx += x[j] * W[i][j]\n",
    "    y.append(Wx + b[i])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "Now that you understand how to do basic computations with a simple perceptron model manually, we will proceed to implement the same model above using matrix-vector operations utilizing PyTorch functions. Organizing the computations in matrix-vector format notation makes it simpler to understand and implement neural network models. \n",
    "\n",
    "Write the code to create the same output vector $\\mathbf{y}$ as above by expressing the above computations as matrix-vector multiplications and summation with a bias vector using code vectorization in PyTorch. You should get the same output as above, up to floating-point errors. Again use Python variables for the mathematical variables, but use PyTorch arrays for vectors and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 58.8380,  81.9780, 135.7120], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(x)\n",
    "W1 = torch.tensor(W1)\n",
    "W2 = torch.tensor(W2)\n",
    "W3 = torch.tensor(W3)\n",
    "W = torch.tensor(W)\n",
    "b = torch.tensor(b)\n",
    "\n",
    "net = torch.nn.Linear(4, 3)\n",
    "net.weight = torch.nn.Parameter(W)\n",
    "net.bias = torch.nn.Parameter(b)\n",
    "print(net(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "### <font color=blue>Question 3: Explain what each of the dimensions of the matrix of weights $\\mathbf{W}$ and the vector of biases $\\mathbf{b}$ represent?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: The number of rows of W (3) is for the 3 outputs, and the number of columns (4) is for the 4 inputs. The number of rows of b (3) is for the 3 outputs, and the number of columns (1) is for the one value the dot product of the inputs and weights calculates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Question 4: What is the total number of parameters for this model?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: There are 15 parameters for this model, 3 sets of 4-value weights and 3 biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "## More Layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 24
   },
   "source": [
    "A single-layer perceptron network still represents a linear classifier, even if we were to use nonlinear activation functions. This limitation can be overcome by multi-layer neural networks in combination with nonlinear activation functions, which introduce one or more 'hidden' layers between the input and output layers. Multi-layer neural networks are composed of several simple artificial neurons such that the output of one acts as the input of another. A multi-layer neural network can be represented by a composition function. For a two-layer network with only one output, the composition function can be written as\n",
    "\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f^{(2)}\\left(\\sum_{k=1}^{h}W_{kj}^{(2)}*f^{(1)}\\left(\\sum_{i=1}^{n}W_{ik}^{(1)}*x_i+b_k^{(1)}\\right)+b_j^{(2)}\\right)\n",
    "\\end{equation}\n",
    "where $h$ is the number of units in the hidden layer and the set of unknown parameters is $\\Theta = \\{\\mathbf{W}^{(1)} \\in R^{n \\times h}, \\mathbf{W}^{(2)} \\in R^{h \\times 1}\\}$. In general, for $L - 1$ hidden layers the composition function, omitting the bias terms, can be written as\n",
    "\\begin{equation}\n",
    "y_j (\\mathbf{x}; \\Theta) = f^{(L)}\\left(\\sum_k W_{kj}^{L}*f^{L-1}\\left(\\sum_{l}W_{lk}^{L - 1}* f^{L - 2}\\left( \\cdots f^{1}\\left(\\sum_{i}W_{iz}^{1}*x_i \\right)\\right) \\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The figure below illustrates a feed-forward neural network composed of an input layer, a hidden layer, and an output layer. In this illustration, the multi-layer neural network has one input layer and one output unit. In most models, the number of hidden layers and output units is more than one.\n",
    "\n",
    "We will now see how to add an additional layer to our model and then how to generalize to any number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add another layer we need another set of weights and biases. \n",
    "\n",
    "W_2 = [[-0.3, 0.66, 0.98],\n",
    "       [0.58, -0.4, 0.38],\n",
    "       [0.87, 0.69, -0.4]]\n",
    "\n",
    "b_2 = [3.9, 8.2, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "origin_pos": 28
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[173.73363999999998, 60.15480000000001, 54.43097999999999]\n"
     ]
    }
   ],
   "source": [
    "x2 = y\n",
    "y = []\n",
    "for i in range(len(W_2)):\n",
    "    Wx2 = 0\n",
    "    for j in range(len(x2)):\n",
    "        Wx2 += x2[j] * W_2[i][j]\n",
    "    y.append(Wx2 + b_2[i])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "### <font color=blue>Question 6: Explain the dimensions of the weight matrix for the second layer with respect to the dimensions of the previous layer and the number of artificial neurons in the second layer. Or why are the dimensions of the weight matrix of the second layer 3x3?</font>\n",
    "\n",
    "Ans: Since the number of output values of the previous layer is 3, and the desired number of output values after this layer is still 3, taking the dot product of the previous outputs and this layer's weight matrix gives us the correct values to then take through the biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers to Objects\n",
    "\n",
    "Now, we have a feed-forward model (with an input layer, one hidden layer, and one output layer with 3 outputs) capable of processing a batch of data. It would be cumbersome and redundant if we had to keep writing the same code for hundreds of layers. So, to make our code more modular, easier to manage, and less redundant we will represent layers using an object-oriented programming paradigm. Let's define classes for representing our layers.\n",
    "\n",
    "All layer objects should have an `output` instance attribute.  Use good object-oriented practices to avoid code duplication.  To initialize an instance attribute in Python, write `self.attribute_name = attribute_value` in the initializer (`__init__` method).  Don't mention the variable at the top of the class as we would usually do in Java -- this is how you define static attributes in Python.\n",
    "\n",
    "Rather than each layer taking PyTorch arrays as inputs, it should take `Layer`s as inputs, with each layer having its own name. For example, if your network would take $\\mathbf{x}$, $\\mathbf{W}$, and $\\mathbf{b}$ as inputs, you should have attributes `self.x`, `self.W`, and `self.b`.  Then, when you need the values of these inputs, go back and read the output of the previous layer.  For example, if your layer needs the value of $\\mathbf{W}$, you could read `self.W.output` to get it.\n",
    "\n",
    "Two more Python OO hints: (1) `class MyClass1(MyClass2)` is not a constructor call. It is specifying the inheritance relationship. The Java equivalent is `class MyClass1 extends MyClass2`. So you don't want to add arguments on this line. An easy mistake to make.  (2) You must use `self.` every time you access an instance variable in Python. This is how the language was designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, output_shape):\n",
    "        \"\"\"\n",
    "        :param output_shape (tuple): the shape of the output array\n",
    "            When this is a single number, it gives the number of output neurons.\n",
    "            When this is an array, it gives the dimensions of the array of output neurons.\n",
    "        \"\"\"\n",
    "        self.output = torch.zeros(output_shape)\n",
    "        \n",
    "\n",
    "class Input(Layer):\n",
    "    def __init__(self, input_shape):\n",
    "        \"\"\"\n",
    "        :param input_shape (int or tuple): the shape of the input array the network will take\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        Layer.__init__(self, input_shape)\n",
    "\n",
    "    def set(self, value):\n",
    "        \"\"\"\n",
    "        Set the `output` of this array to have value `value`.\n",
    "        Raise an error if the size of value is unexpected. An `assert()` is fine for this.\n",
    "        \"\"\"\n",
    "        if isinstance(self.input_shape, int):\n",
    "            assert(value.ndim == 1)\n",
    "            assert(value.size == self.input_shape)\n",
    "        else:\n",
    "            assert(value.shape == self.input_shape)\n",
    "        self.output = torch.from_numpy(value)\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"This layer's values do not change during forward propagation.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, x, W, b):\n",
    "        \"\"\"\n",
    "        :param x (Layer): the inputs of the layer\n",
    "        :param W (Layer): the weights of the layer\n",
    "        :param b (Layer): the biases of the layer\n",
    "        \n",
    "        Raise an error if any of the argument's size do not match as you would expect.\n",
    "        \"\"\"\n",
    "        assert(x.output.ndim == 1)\n",
    "        assert(x.output.shape[0] == W.output.shape[1])\n",
    "        assert(b.output.ndim == 1)\n",
    "        assert(W.output.shape[0] == b.output.shape[0])\n",
    "        Layer.__init__(self, b.output.shape)\n",
    "        self.x = x\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set this layer's output based on the outputs of the layers that feed into it.\n",
    "        \"\"\"\n",
    "        net = torch.nn.Linear(self.x.output.shape[0], self.b.output.shape[0])\n",
    "        net.weight = torch.nn.Parameter(self.W.output)\n",
    "        net.bias = torch.nn.Parameter(self.b.output)\n",
    "        self.output = net(self.x.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " output of hidden layer \n",
      " tensor([ 58.8380,  81.9780, 135.7120], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# This example uses your classes to test the output of the entire network.\n",
    "#\n",
    "# You may change this example to match your own implementation if you wish.\n",
    "\n",
    "x_layer = Input(4)\n",
    "x_layer.set(np.array(x))\n",
    "W_layer = Input((3, 4))\n",
    "W_layer.set(np.array(W))\n",
    "b_layer = Input(3)\n",
    "b_layer.set(np.array(b))\n",
    "linear_layer = Linear(x_layer, W_layer, b_layer)\n",
    "\n",
    "linear_layer.forward()\n",
    "print('\\n output of hidden layer \\n', linear_layer.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this lab... except for the two **required** parting questions:\n",
    "\n",
    "### <font color=blue>Question 7: Summarize what you learned during this lab.\n",
    "\n",
    "Ans: I learned how to implement a perceptron with both PyTorch and normal Python lists and loops. I also learned how to implement it with mutiple layers and outputs, as well as how to make it more modular and manageable with classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Question 8: Describe what you liked about this lab *or* what could be improved.\n",
    "\n",
    "Ans: I liked that we had to do implement the perceptron with regular Python lists and for loops first to help us better understand what it does logically. Some small details could have been clarified, such as whether we were supposed to implement the classes with pytorch or go back to using the regular Python lists and for loops."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
